<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cybersecurity ML Pipeline - Implementation Guide</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/themes/prism-tomorrow.min.css" rel="stylesheet">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/components/prism-core.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.29.0/plugins/autoloader/prism-autoloader.min.js"></script>
    <style>
        .code-container {
            max-height: 500px;
            overflow-y: auto;
        }
        .tab-content {
            display: none;
        }
        .tab-content.active {
            display: block;
        }
        .tab-button.active {
            background-color: #3b82f6;
            color: white;
        }
        .section-content {
            display: none;
            opacity: 0;
            transition: opacity 0.3s ease-in-out;
        }
        .section-content.active {
            display: block;
            opacity: 1;
        }
        .nav-btn.active {
            background-color: #3b82f6 !important;
            color: white !important;
        }
        .nav-btn {
            transition: all 0.2s ease-in-out;
        }
        .nav-btn:hover {
            background-color: #f3f4f6;
            transform: translateY(-1px);
        }
        .nav-btn.active:hover {
            background-color: #2563eb !important;
        }
    </style>
</head>
<body class="bg-gray-50 min-h-screen">
    <!-- Header -->
    <header class="bg-gradient-to-r from-blue-900 to-purple-900 text-white py-8">
        <div class="container mx-auto px-6">
            <h1 class="text-4xl font-bold mb-2">Cybersecurity ML Pipeline</h1>
            <p class="text-xl opacity-90">Production-Ready Threat Detection System</p>
            <div class="mt-4">
                <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/37776e65-d36a-4e14-9c86-adc8a25e3570.png" alt="Cybersecurity dashboard showing network traffic analysis, threat detection graphs, and real-time monitoring displays with dark blue and purple gradient background" class="w-full rounded-lg opacity-80">
            </div>
        </div>
    </header>

    <!-- Navigation -->
    <nav class="bg-white shadow-md sticky top-0 z-50">
        <div class="container mx-auto px-6">
            <div class="flex space-x-8 overflow-x-auto py-4">
                <button onclick="showSection('overview')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg bg-blue-100 text-blue-700 font-medium active">Overview</button>
                <button onclick="showSection('structure')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">Project Structure</button>
                <button onclick="showSection('preprocessing')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">Data Pipeline</button>
                <button onclick="showSection('models')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">ML Models</button>
                <button onclick="showSection('api')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">API & Serving</button>
                <button onclick="showSection('agents')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">Agentic AI</button>
                <button onclick="showSection('deployment')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">Deployment</button>
                <button onclick="showSection('monitoring')" class="nav-btn whitespace-nowrap px-4 py-2 rounded-lg hover:bg-gray-100">Monitoring</button>
            </div>
        </div>
    </nav>

    <div class="container mx-auto px-6 py-8">
        <!-- Overview Section -->
        <section id="overview" class="section-content active">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">System Architecture Overview</h2>
                <div class="grid md:grid-cols-2 gap-8">
                    <div>
                        <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/8632b1a6-c2b4-4505-972c-5b60c4b81527.png" alt="System architecture diagram showing data flow from ingestion through feature engineering, model training, serving, and monitoring with interconnected components in a modern technical style" class="w-full rounded-lg shadow-md">
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-4">Key Components</h3>
                        <ul class="space-y-3">
                            <li class="flex items-start">
                                <span class="bg-blue-100 text-blue-700 px-2 py-1 rounded text-sm font-medium mr-3">Data</span>
                                <span>Automated ingestion with validation (S3, Kafka, CloudWatch)</span>
                            </li>
                            <li class="flex items-start">
                                <span class="bg-green-100 text-green-700 px-2 py-1 rounded text-sm font-medium mr-3">ML</span>
                                <span>Ensemble models with continuous retraining</span>
                            </li>
                            <li class="flex items-start">
                                <span class="bg-purple-100 text-purple-700 px-2 py-1 rounded text-sm font-medium mr-3">API</span>
                                <span>FastAPI serving with async streaming alerts</span>
                            </li>
                            <li class="flex items-start">
                                <span class="bg-orange-100 text-orange-700 px-2 py-1 rounded text-sm font-medium mr-3">Agents</span>
                                <span>Autonomous triage and maintenance assistants</span>
                            </li>
                        </ul>
                        
                        <div class="mt-6 p-4 bg-yellow-50 border border-yellow-200 rounded-lg">
                            <h4 class="font-semibold text-yellow-800 mb-2">Quick Start Commands</h4>
                            <div class="text-sm text-yellow-700 space-y-1">
                                <code class="block bg-yellow-100 px-2 py-1 rounded">python3 -m venv venv && source venv/bin/activate</code>
                                <code class="block bg-yellow-100 px-2 py-1 rounded">pip install -r requirements.txt</code>
                                <code class="block bg-yellow-100 px-2 py-1 rounded">python src/train.py --data data/cybersecurity_data.csv</code>
                                <code class="block bg-yellow-100 px-2 py-1 rounded">uvicorn src.app:app --reload</code>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Project Structure Section -->
        <section id="structure" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">Project Structure & Setup</h2>
                <div class="grid lg:grid-cols-2 gap-8">
                    <div>
                        <h3 class="text-xl font-semibold mb-4">Directory Structure</h3>
                        <div class="code-container bg-gray-900 rounded-lg p-4">
                            <pre><code class="language-bash">Cybersecurity-Suspicious-Web-Threat-Interactions/
│
├── data/                      # Raw and processed datasets
│   ├── raw/
│   │   └── cybersecurity_data.csv
│   ├── processed/
│   │   └── features.parquet
│   └── validation/
│       └── schema.json
│
├── notebooks/                 # Jupyter notebooks for EDA
│   ├── 01_EDA.ipynb
│   ├── 02_Feature_Engineering.ipynb
│   └── 03_Model_Evaluation.ipynb
│
├── src/                      # Source code
│   ├── __init__.py
│   ├── app.py               # FastAPI application
│   ├── preprocess.py        # Data preprocessing
│   ├── train.py             # Model training
│   ├── agents/              # Agentic AI components
│   │   ├── triage_agent.py
│   │   └── maintenance_agent.py
│   ├── models/              # Model definitions
│   │   ├── ensemble.py
│   │   ├── isolation_forest.py
│   │   └── autoencoder.py
│   └── utils/               # Utility functions
│       ├── monitoring.py
│       └── security.py
│
├── deploy/                   # Deployment configurations
│   ├── Dockerfile
│   ├── docker-compose.yml
│   └── k8s/                 # Kubernetes manifests
│       ├── deployment.yaml
│       ├── service.yaml
│       └── ingress.yaml
│
├── infra/                    # Infrastructure as code
│   ├── airflow/             # Airflow DAGs
│   │   ├── training_dag.py
│   │   └── monitoring_dag.py
│   └── terraform/           # Infrastructure provisioning
│       └── main.tf
│
├── tests/                    # Unit and integration tests
│   ├── test_preprocess.py
│   ├── test_models.py
│   └── test_api.py
│
├── configs/                  # Configuration files
│   ├── model_config.yaml
│   ├── feature_config.yaml
│   └── deployment_config.yaml
│
├── requirements.txt          # Python dependencies
├── requirements-dev.txt      # Development dependencies
├── Makefile                 # Common commands
├── .github/                 # CI/CD workflows
│   └── workflows/
│       └── ci.yml
└── README.md                # Project documentation</code></pre>
                        </div>
                    </div>
                    <div>
                        <h3 class="text-xl font-semibold mb-4">Requirements.txt</h3>
                        <div class="code-container bg-gray-900 rounded-lg p-4">
                            <pre><code class="language-python"># Core ML Libraries
pandas==2.1.0
numpy==1.24.0
scikit-learn==1.3.0
lightgbm==4.0.0
xgboost==1.7.0
tensorflow==2.13.0

# MLOps & Experiment Tracking
mlflow==2.7.0
feast==0.34.0
great-expectations==0.17.0
evidently==0.4.0

# API & Serving
fastapi==0.103.0
uvicorn[standard]==0.23.0
pydantic==2.3.0
redis==4.6.0

# Data Processing
pyarrow==13.0.0
kafka-python==2.0.2
boto3==1.28.0

# Monitoring & Observability
prometheus-client==0.17.0
sentry-sdk==1.32.0

# Explainability
shap==0.42.0
lime==0.2.0.1

# Agentic AI
langchain==0.0.284
openai==0.28.0
chromadb==0.4.0

# Security
cryptography==41.0.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4

# Development & Testing
pytest==7.4.0
black==23.7.0
flake8==6.0.0
mypy==1.5.0

# Visualization
matplotlib==3.7.0
seaborn==0.12.0
plotly==5.15.0</code></pre>
                        </div>
                        
                        <div class="mt-6">
                            <h3 class="text-lg font-semibold mb-3">Setup Commands</h3>
                            <div class="bg-gray-100 rounded-lg p-4">
                                <div class="space-y-2 text-sm">
                                    <div><strong>1. Clone and Setup:</strong></div>
                                    <code class="block bg-white px-3 py-2 rounded">git clone <repo-url> && cd Cybersecurity-Suspicious-Web-Threat-Interactions</code>
                                    <code class="block bg-white px-3 py-2 rounded">python3 -m venv venv && source venv/bin/activate</code>
                                    <code class="block bg-white px-3 py-2 rounded">pip install -r requirements.txt</code>
                                    
                                    <div class="mt-4"><strong>2. Initialize MLflow:</strong></div>
                                    <code class="block bg-white px-3 py-2 rounded">mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0</code>
                                    
                                    <div class="mt-4"><strong>3. Run Tests:</strong></div>
                                    <code class="block bg-white px-3 py-2 rounded">pytest tests/ -v</code>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Data Pipeline Section -->
        <section id="preprocessing" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">Data Pipeline & Preprocessing</h2>
                
                <!-- Tab Navigation -->
                <div class="flex space-x-4 mb-6 border-b">
                    <button onclick="showTab('preprocess', 'ingestion')" class="tab-button px-4 py-2 font-medium text-blue-600 border-b-2 border-blue-600 active">Data Ingestion</button>
                    <button onclick="showTab('preprocess', 'validation')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Validation</button>
                    <button onclick="showTab('preprocess', 'features')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Feature Engineering</button>
                </div>

                <!-- Ingestion Tab -->
                <div id="preprocess-ingestion" class="tab-content active">
                    <h3 class="text-xl font-semibold mb-4">Data Ingestion Pipeline</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/preprocess.py
import pandas as pd
import numpy as np
import pyarrow.parquet as pq
from typing import Dict, List, Optional
import logging
from datetime import datetime
import boto3
from kafka import KafkaConsumer
import json

class DataIngestionPipeline:
    """
    Handles batch and streaming data ingestion with validation
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.s3_client = boto3.client('s3') if config.get('use_s3') else None
        self.setup_logging()
    
    def setup_logging(self):
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def ingest_batch_csv(self, file_path: str) -> pd.DataFrame:
        """
        Ingest CSV data with preprocessing and normalization
        """
        try:
            # Load data
            df = pd.read_csv(file_path)
            self.logger.info(f"Loaded {len(df)} records from {file_path}")
            
            # Normalize data
            df = self._normalize_data(df)
            
            # Save to parquet for faster access
            parquet_path = file_path.replace('.csv', '.parquet')
            df.to_parquet(parquet_path, index=False)
            self.logger.info(f"Saved processed data to {parquet_path}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"Error ingesting batch data: {e}")
            raise
    
    def _normalize_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Normalize timestamps, IPs, ports, and labels
        """
        # Convert timestamp to datetime
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')
        
        # Normalize IP addresses
        if 'src_ip' in df.columns:
            df['src_ip'] = df['src_ip'].astype(str)
        if 'dst_ip' in df.columns:
            df['dst_ip'] = df['dst_ip'].astype(str)
        
        # Ensure ports are numeric
        port_columns = ['src_port', 'dst_port']
        for col in port_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Normalize categorical labels
        if 'label' in df.columns:
            df['label'] = df['label'].map({
                'malicious': 1, 'suspicious': 1, 'anomaly': 1,
                'normal': 0, 'benign': 0, 'legitimate': 0
            }).fillna(0)
        
        # Handle missing values
        df = df.fillna(0)
        
        return df

# Usage example
if __name__ == "__main__":
    config = {
        'use_s3': False,
        'kafka_servers': ['localhost:9092']
    }
    
    pipeline = DataIngestionPipeline(config)
    df = pipeline.ingest_batch_csv('data/raw/cybersecurity_data.csv')
    print(f"Processed {len(df)} records")</code></pre>
                    </div>
                </div>

                <!-- Validation Tab -->
                <div id="preprocess-validation" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Data Validation with Great Expectations</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/validation.py
import great_expectations as ge
from great_expectations.core import ExpectationSuite
import pandas as pd
from typing import Dict, List
import logging

class DataValidator:
    """
    Data validation pipeline using Great Expectations
    """
    
    def __init__(self, context_root_dir: str = "./gx"):
        self.context = ge.get_context(context_root_dir=context_root_dir)
        self.logger = logging.getLogger(__name__)
    
    def create_cybersecurity_expectations(self) -> ExpectationSuite:
        """
        Create expectation suite for cybersecurity data
        """
        suite = self.context.create_expectation_suite(
            "cybersecurity_data_suite", 
            overwrite_existing=True
        )
        
        # Core column expectations
        expectations = [
            # Timestamp expectations
            {
                "expectation_type": "expect_column_to_exist",
                "kwargs": {"column": "timestamp"}
            },
            # IP address expectations
            {
                "expectation_type": "expect_column_to_exist",
                "kwargs": {"column": "src_ip"}
            },
            # Port expectations
            {
                "expectation_type": "expect_column_values_to_be_between",
                "kwargs": {
                    "column": "src_port",
                    "min_value": 0,
                    "max_value": 65535
                }
            }
        ]
        
        # Add expectations to suite
        for exp in expectations:
            suite.add_expectation(**exp)
        
        self.context.save_expectation_suite(suite)
        return suite

# Usage
if __name__ == "__main__":
    validator = DataValidator()
    suite = validator.create_cybersecurity_expectations()
    df = pd.read_csv("data/cybersecurity_data.csv")
    results = validator.validate_batch(df)
    
    if not results["success"]:
        print("Validation failed! Check logs for details.")
    else:
        print("Data validation passed!")</code></pre>
                    </div>
                </div>

                <!-- Features Tab -->
                <div id="preprocess-features" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Feature Engineering Pipeline</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/feature_engineering.py
import pandas as pd
import numpy as np
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, LabelEncoder
from typing import Dict, List, Optional
import joblib

class CyberSecurityFeatureEngineer(BaseEstimator, TransformerMixin):
    """
    Custom feature engineering for cybersecurity data
    """
    
    def __init__(self, geoip_db_path: Optional[str] = None):
        self.geoip_db_path = geoip_db_path
        self.geoip_reader = None
        self.ip_encoders = {}
        self.feature_names = []
    
    def fit(self, X: pd.DataFrame, y=None):
        """
        Fit the feature engineer on training data
        """
        # Fit label encoders for categorical features
        categorical_cols = ['protocol', 'src_ip_country', 'dst_ip_country']
        for col in categorical_cols:
            if col in X.columns:
                encoder = LabelEncoder()
                encoder.fit(X[col].fillna('unknown'))
                self.ip_encoders[col] = encoder
        
        return self
    
    def transform(self, X: pd.DataFrame) -> pd.DataFrame:
        """
        Transform data with engineered features
        """
        X_transformed = X.copy()
        
        # Time-based features
        X_transformed = self._add_time_features(X_transformed)
        
        # Traffic analysis features
        X_transformed = self._add_traffic_features(X_transformed)
        
        # Network behavior features
        X_transformed = self._add_network_features(X_transformed)
        
        # Store feature names
        self.feature_names = list(X_transformed.columns)
        
        return X_transformed
    
    def _add_time_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Extract time-based features"""
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df['hour'] = df['timestamp'].dt.hour
            df['day_of_week'] = df['timestamp'].dt.dayofweek
            df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)
        return df
    
    def _add_traffic_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compute traffic analysis features"""
        df['bytes_ratio'] = (
            df['bytes_sent'] / (df['bytes_sent'] + df['bytes_received'] + 1e-6)
        )
        df['total_bytes'] = df['bytes_sent'] + df['bytes_received']
        return df
    
    def _add_network_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Add network behavior features"""
        df['is_well_known_port'] = (df['dst_port'] <= 1023).astype(int)
        return df

def create_feature_pipeline() -> Pipeline:
    """Create complete feature engineering pipeline"""
    pipeline = Pipeline([
        ('feature_engineer', CyberSecurityFeatureEngineer()),
        ('scaler', StandardScaler())
    ])
    return pipeline</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Models Section -->
        <section id="models" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">ML Models & Training</h2>
                
                <!-- Tab Navigation -->
                <div class="flex space-x-4 mb-6 border-b">
                    <button onclick="showTab('models', 'ensemble')" class="tab-button px-4 py-2 font-medium text-blue-600 border-b-2 border-blue-600 active">Ensemble Models</button>
                    <button onclick="showTab('models', 'unsupervised')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Unsupervised</button>
                    <button onclick="showTab('models', 'training')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Training Pipeline</button>
                </div>

                <!-- Ensemble Tab -->
                <div id="models-ensemble" class="tab-content active">
                    <h3 class="text-xl font-semibold mb-4">Ensemble Model Architecture</h3>
                    <div class="mb-4">
                        <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/7cc79c85-d6c7-441c-8281-1db5987997c6.png" alt="Ensemble model architecture diagram showing multiple model types including isolation forest, autoencoder, and gradient boosting models feeding into a meta-learner with stacking approach" class="w-full rounded-lg shadow-md">
                    </div>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/models/ensemble.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.metrics import roc_auc_score, precision_recall_curve
import lightgbm as lgb
import xgboost as xgb
from tensorflow import keras
import mlflow
from typing import Dict, List, Tuple, Any
import joblib

class CyberSecurityEnsemble:
    """
    Advanced ensemble model for cybersecurity threat detection
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        self.models = {}
        self.meta_model = None
        self.feature_importance = {}
        
    def _default_config(self) -> Dict:
        return {
            'isolation_forest': {
                'contamination': 0.1,
                'n_estimators': 100,
                'random_state': 42
            },
            'lightgbm': {
                'objective': 'binary',
                'metric': 'auc',
                'boosting_type': 'gbdt',
                'num_leaves': 31,
                'learning_rate': 0.05,
                'feature_fraction': 0.9,
                'random_state': 42
            },
            'xgboost': {
                'objective': 'binary:logistic',
                'eval_metric': 'auc',
                'max_depth': 6,
                'learning_rate': 0.1,
                'random_state': 42
            }
        }
    
    def fit(self, X_train: np.ndarray, y_train: np.ndarray, 
            X_val: np.ndarray = None, y_val: np.ndarray = None) -> Dict:
        """Train ensemble of models"""
        results = {}
        
        with mlflow.start_run(run_name="cybersecurity_ensemble"):
            # 1. Isolation Forest (Unsupervised)
            print("Training Isolation Forest...")
            iso_forest = IsolationForest(**self.config['isolation_forest'])
            iso_forest.fit(X_train)
            self.models['isolation_forest'] = iso_forest
            
            # 2. LightGBM (Supervised)
            print("Training LightGBM...")
            lgb_train = lgb.Dataset(X_train, label=y_train)
            lgb_model = lgb.train(
                self.config['lightgbm'],
                lgb_train,
                num_boost_round=1000
            )
            self.models['lightgbm'] = lgb_model
            
            # 3. XGBoost (Supervised)
            print("Training XGBoost...")
            xgb_model = xgb.XGBClassifier(**self.config['xgboost'])
            xgb_model.fit(X_train, y_train)
            self.models['xgboost'] = xgb_model
            
            # Validation if provided
            if X_val is not None:
                val_predictions = self.predict_proba(X_val)
                val_auc = roc_auc_score(y_val, val_predictions)
                results['validation_auc'] = val_auc
                mlflow.log_metric("validation_auc", val_auc)
            
        return results
    
    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Predict probabilities using ensemble"""
        # Simple averaging for demonstration
        predictions = []
        
        # LightGBM predictions
        lgb_preds = self.models['lightgbm'].predict(X)
        predictions.append(lgb_preds)
        
        # XGBoost predictions
        xgb_preds = self.models['xgboost'].predict_proba(X)[:, 1]
        predictions.append(xgb_preds)
        
        return np.mean(predictions, axis=0)
    
    def predict(self, X: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """Binary predictions"""
        probabilities = self.predict_proba(X)
        return (probabilities >= threshold).astype(int)

# Usage example
if __name__ == "__main__":
    from sklearn.model_selection import train_test_split
    from sklearn.datasets import make_classification
    
    X, y = make_classification(n_samples=10000, n_features=20, n_classes=2, 
                             n_informative=15, random_state=42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    ensemble = CyberSecurityEnsemble()
    results = ensemble.fit(X_train, y_train, X_test, y_test)
    
    predictions = ensemble.predict_proba(X_test)
    test_auc = roc_auc_score(y_test, predictions)
    print(f"Test AUC: {test_auc:.4f}")</code></pre>
                    </div>
                </div>

                <!-- Unsupervised Tab -->
                <div id="models-unsupervised" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Unsupervised Anomaly Detection</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/models/unsupervised.py
import numpy as np
import pandas as pd
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from tensorflow import keras
from typing import Dict, Tuple, List
import joblib

class UnsupervisedAnomalyDetector:
    """
    Collection of unsupervised models for anomaly detection
    """
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.thresholds = {}
        
    def fit_isolation_forest(self, X: np.ndarray, contamination: float = 0.1) -> Dict:
        """Fit Isolation Forest model"""
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
        
        iso_forest = IsolationForest(
            contamination=contamination,
            n_estimators=200,
            max_samples='auto',
            random_state=42,
            n_jobs=-1
        )
        iso_forest.fit(X_scaled)
        
        self.models['isolation_forest'] = iso_forest
        self.scalers['isolation_forest'] = scaler
        
        scores = iso_forest.decision_function(X_scaled)
        threshold = np.percentile(scores, contamination * 100)
        self.thresholds['isolation_forest'] = threshold
        
        return {
            'model': iso_forest,
            'scaler': scaler,
            'threshold': threshold,
            'scores': scores
        }
    
    def predict_anomalies(self, X: np.ndarray, method: str = 'isolation_forest') -> Dict:
        """Predict anomalies using specified method"""
        results = {}
        
        if method == 'isolation_forest':
            if 'isolation_forest' in self.models:
                X_scaled = self.scalers['isolation_forest'].transform(X)
                iso_scores = self.models['isolation_forest'].decision_function(X_scaled)
                iso_anomalies = iso_scores < self.thresholds['isolation_forest']
                results['isolation_forest'] = {
                    'scores': iso_scores,
                    'anomalies': iso_anomalies,
                    'anomaly_ratio': np.mean(iso_anomalies)
                }
        
        return results

# Usage example
if __name__ == "__main__":
    from sklearn.datasets import make_blobs
    
    X_normal, _ = make_blobs(n_samples=1000, centers=3, n_features=10, 
                            cluster_std=1.0, random_state=42)
    X_anomalies = np.random.uniform(low=-6, high=6, size=(50, 10))
    X = np.vstack([X_normal, X_anomalies])
    
    detector = UnsupervisedAnomalyDetector()
    iso_results = detector.fit_isolation_forest(X, contamination=0.05)
    predictions = detector.predict_anomalies(X, method='isolation_forest')
    
    print("Anomaly detection completed!")</code></pre>
                    </div>
                </div>

                <!-- Training Tab -->
                <div id="models-training" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Training Pipeline with MLflow</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/train.py
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import roc_auc_score, precision_recall_curve, classification_report
import mlflow
import argparse
import yaml
import joblib
from pathlib import Path
import logging
from typing import Dict, Tuple

from feature_engineering import create_feature_pipeline
from models.ensemble import CyberSecurityEnsemble
from validation import DataValidator

class ModelTrainingPipeline:
    """Complete training pipeline with MLflow tracking"""
    
    def __init__(self, config_path: str = "configs/model_config.yaml"):
        self.config = self._load_config(config_path)
        self.setup_logging()
        self.setup_mlflow()
        
    def _load_config(self, config_path: str) -> Dict:
        """Load configuration from YAML file"""
        try:
            with open(config_path, 'r') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            return self._default_config()
    
    def _default_config(self) -> Dict:
        """Default configuration"""
        return {
            'data': {
                'test_size': 0.2,
                'validation_size': 0.2,
                'random_state': 42
            },
            'training': {
                'cv_folds': 5,
                'scoring': 'roc_auc'
            },
            'mlflow': {
                'experiment_name': 'cybersecurity_threat_detection',
                'tracking_uri': 'sqlite:///mlflow.db'
            }
        }
    
    def setup_logging(self):
        """Setup logging configuration"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
    
    def setup_mlflow(self):
        """Setup MLflow tracking"""
        mlflow.set_tracking_uri(self.config['mlflow']['tracking_uri'])
        
        try:
            experiment = mlflow.get_experiment_by_name(
                self.config['mlflow']['experiment_name']
            )
            if experiment is None:
                experiment_id = mlflow.create_experiment(
                    self.config['mlflow']['experiment_name']
                )
            else:
                experiment_id = experiment.experiment_id
                
            mlflow.set_experiment(experiment_id=experiment_id)
            
        except Exception as e:
            self.logger.error(f"MLflow setup error: {e}")
    
    def run_training_pipeline(self, data_path: str):
        """Run complete training pipeline"""
        try:
            # Load data
            self.logger.info(f"Loading data from {data_path}")
            df = pd.read_csv(data_path)
            
            # Prepare features
            X = df.drop('label', axis=1)
            y = df['label'].values
            
            # Create and fit feature pipeline
            self.feature_pipeline = create_feature_pipeline()
            X_processed = self.feature_pipeline.fit_transform(X)
            
            # Split data
            X_train, X_test, y_train, y_test = train_test_split(
                X_processed, y,
                test_size=self.config['data']['test_size'],
                random_state=self.config['data']['random_state'],
                stratify=y
            )
            
            # Train model
            ensemble = CyberSecurityEnsemble()
            results = ensemble.fit(X_train, y_train, X_test, y_test)
            
            # Evaluate model
            predictions = ensemble.predict_proba(X_test)
            test_auc = roc_auc_score(y_test, predictions)
            
            self.logger.info(f"Training completed! Test AUC: {test_auc:.4f}")
            
            # Save models
            Path("models").mkdir(exist_ok=True)
            joblib.dump(self.feature_pipeline, "models/feature_pipeline.joblib")
            
            return ensemble, {'test_auc': test_auc}
            
        except Exception as e:
            self.logger.error(f"Training pipeline failed: {e}")
            raise

def main():
    """Main training script"""
    parser = argparse.ArgumentParser(description='Train cybersecurity threat detection model')
    parser.add_argument('--data', required=True, help='Path to training data')
    parser.add_argument('--config', default='configs/model_config.yaml', 
                       help='Path to config file')
    
    args = parser.parse_args()
    
    pipeline = ModelTrainingPipeline(args.config)
    ensemble, metrics = pipeline.run_training_pipeline(args.data)
    
    print("Training completed!")
    print(f"Final test AUC: {metrics['test_auc']:.4f}")

if __name__ == "__main__":
    main()</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- API Section -->
        <section id="api" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">API & Model Serving</h2>
                <div class="mb-4">
                    <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/531c43c9-3300-4257-8ca3-2ddf76596794.png" alt="API architecture diagram showing FastAPI endpoints, model serving infrastructure, Redis caching, and real-time streaming components with monitoring dashboards" class="w-full rounded-lg shadow-md">
                </div>
                <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                    <pre><code class="language-python"># src/app.py
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field, validator
import numpy as np
import pandas as pd
import joblib
import redis
import json
import logging
import time
from typing import Dict, List, Optional, Any
from datetime import datetime
import uvicorn
from prometheus_client import Counter, Histogram, generate_latest
import hashlib

from models.ensemble import CyberSecurityEnsemble

# Metrics
PREDICTION_REQUESTS = Counter('prediction_requests_total', 'Total prediction requests')
PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')
ANOMALY_DETECTIONS = Counter('anomaly_detections_total', 'Total anomaly detections')

# Initialize FastAPI app
app = FastAPI(
    title="Cybersecurity Threat Detection API",
    description="Production-ready ML API for detecting cybersecurity threats",
    version="1.0.0"
)

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Global variables
model = None
feature_pipeline = None
redis_client = None

# Pydantic models
class NetworkEvent(BaseModel):
    """Input model for network event data"""
    timestamp: Optional[str] = Field(default_factory=lambda: datetime.now().isoformat())
    src_ip: str = Field(..., description="Source IP address")
    dst_ip: str = Field(..., description="Destination IP address")
    src_port: int = Field(..., ge=0, le=65535, description="Source port")
    dst_port: int = Field(..., ge=0, le=65535, description="Destination port")
    protocol: str = Field(..., description="Network protocol")
    bytes_sent: int = Field(..., ge=0, description="Bytes sent")
    bytes_received: int = Field(..., ge=0, description="Bytes received")
    packets_sent: int = Field(..., ge=0, description="Packets sent")
    packets_received: int = Field(..., ge=0, description="Packets received")
    
    @validator('src_ip', 'dst_ip')
    def validate_ip(cls, v):
        import ipaddress
        try:
            ipaddress.ip_address(v)
            return v
        except ValueError:
            raise ValueError(f"Invalid IP address: {v}")

class PredictionResponse(BaseModel):
    """Prediction response model"""
    event_id: str
    threat_score: float = Field(..., ge=0, le=1)
    is_suspicious: bool
    confidence: float = Field(..., ge=0, le=1)
    risk_level: str
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())

class HealthResponse(BaseModel):
    """Health check response"""
    status: str
    model_loaded: bool
    redis_connected: bool
    timestamp: str
    uptime_seconds: float

# Startup event
@app.on_event("startup")
async def startup_event():
    """Initialize models and connections on startup"""
    global model, feature_pipeline, redis_client
    
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    try:
        # Load feature pipeline
        logger.info("Loading feature pipeline...")
        feature_pipeline = joblib.load("models/feature_pipeline.joblib")
        
        # Load model
        logger.info("Loading ensemble model...")
        model = CyberSecurityEnsemble()
        
        # Initialize Redis connection
        logger.info("Connecting to Redis...")
        redis_client = redis.Redis(
            host='localhost', 
            port=6379, 
            db=0, 
            decode_responses=True,
            socket_timeout=5
        )
        
        logger.info("Startup completed successfully!")
        
    except Exception as e:
        logger.error(f"Startup failed: {e}")

# Utility functions
def calculate_risk_level(threat_score: float) -> str:
    """Calculate risk level based on threat score"""
    if threat_score >= 0.9:
        return "CRITICAL"
    elif threat_score >= 0.7:
        return "HIGH"
    elif threat_score >= 0.5:
        return "MEDIUM"
    else:
        return "LOW"

# API Endpoints
@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check endpoint"""
    start_time = getattr(app.state, 'start_time', time.time())
    
    return HealthResponse(
        status="healthy" if model and feature_pipeline else "unhealthy",
        model_loaded=model is not None,
        redis_connected=redis_client is not None,
        timestamp=datetime.now().isoformat(),
        uptime_seconds=time.time() - start_time
    )

@app.post("/predict", response_model=PredictionResponse)
async def predict_single(event: NetworkEvent):
    """Predict threat for a single network event"""
    start_time = time.time()
    
    try:
        # Generate event ID
        event_string = f"{event.src_ip}:{event.src_port}-{event.dst_ip}:{event.dst_port}"
        event_id = hashlib.md5(event_string.encode()).hexdigest()[:16]
        
        # Convert to DataFrame
        event_df = pd.DataFrame([event.dict()])
        
        # Feature engineering
        features = feature_pipeline.transform(event_df)
        
        # Model prediction
        threat_score = float(model.predict_proba(features)[0])
        is_suspicious = threat_score >= 0.5
        
        # Calculate additional metrics
        risk_level = calculate_risk_level(threat_score)
        confidence = min(abs(threat_score - 0.5) * 2, 1.0)
        
        # Create response
        prediction = PredictionResponse(
            event_id=event_id,
            threat_score=threat_score,
            is_suspicious=is_suspicious,
            confidence=confidence,
            risk_level=risk_level
        )
        
        # Metrics
        PREDICTION_REQUESTS.inc()
        PREDICTION_LATENCY.observe(time.time() - start_time)
        
        if is_suspicious:
            ANOMALY_DETECTIONS.inc()
        
        return prediction
        
    except Exception as e:
        logging.error(f"Prediction error: {e}")
        raise HTTPException(status_code=500, detail=f"Prediction failed: {str(e)}")

@app.get("/metrics")
async def get_metrics():
    """Prometheus metrics endpoint"""
    from fastapi.responses import Response
    return Response(generate_latest(), media_type="text/plain")

@app.get("/model/info")
async def get_model_info():
    """Get model information"""
    if not model:
        raise HTTPException(status_code=503, detail="Model not loaded")
    
    return {
        "model_type": "CyberSecurityEnsemble",
        "version": "1.0.0",
        "last_updated": datetime.now().isoformat()
    }

if __name__ == "__main__":
    app.state.start_time = time.time()
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=8080,
        workers=1,
        reload=False,
        access_log=True
    )</code></pre>
                </div>
            </div>
        </section>

        <!-- Agentic AI Section -->
        <section id="agents" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">Agentic AI Components</h2>
                
                <!-- Tab Navigation -->
                <div class="flex space-x-4 mb-6 border-b">
                    <button onclick="showTab('agents', 'triage')" class="tab-button px-4 py-2 font-medium text-blue-600 border-b-2 border-blue-600 active">Triage Agent</button>
                    <button onclick="showTab('agents', 'maintenance')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Maintenance Agent</button>
                </div>

                <!-- Triage Agent Tab -->
                <div id="agents-triage" class="tab-content active">
                    <h3 class="text-xl font-semibold mb-4">Real-time Triage Agent</h3>
                    <div class="mb-4">
                        <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/b27888a6-03c9-445d-85b0-c9f7637456b7.png" alt="Triage agent workflow diagram showing alert processing, threat enrichment, decision tree, and automated response actions with human escalation paths" class="w-full rounded-lg shadow-md">
                    </div>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/agents/triage_agent.py
import asyncio
import logging
import json
import requests
from typing import Dict, List, Optional, Any
from datetime import datetime, timedelta
from dataclasses import dataclass
from enum import Enum
import redis

class AlertSeverity(Enum):
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"

class ActionType(Enum):
    BLOCK_IP = "block_ip"
    CREATE_INCIDENT = "create_incident"
    ESCALATE_HUMAN = "escalate_human"
    LOG_SUSPICIOUS = "log_suspicious"

@dataclass
class ThreatAlert:
    """Threat alert data structure"""
    id: str
    src_ip: str
    dst_ip: str
    threat_score: float
    severity: AlertSeverity
    timestamp: datetime
    features: Dict[str, Any]
    contributing_factors: List[str]
    raw_event: Dict[str, Any]

class ThreatTriageAgent:
    """
    Autonomous agent for real-time threat triage and response
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.redis_client = redis.Redis(**config.get('redis', {}))
    
    async def process_alert(self, alert: ThreatAlert) -> Dict[str, Any]:
        """Main entry point for processing threat alerts"""
        self.logger.info(f"Processing alert {alert.id} with severity {alert.severity.value}")
        
        try:
            # 1. Enrich alert with additional context
            enrichment = await self.enrich_threat_context(alert)
            
            # 2. Determine appropriate response
            response_plan = await self.determine_response_plan(alert, enrichment)
            
            # 3. Execute response actions
            action_results = await self.execute_response_actions(alert, response_plan)
            
            return {
                "alert_id": alert.id,
                "status": "processed",
                "enrichment": enrichment,
                "actions_taken": action_results,
                "processing_time": datetime.now().isoformat()
            }
            
        except Exception as e:
            self.logger.error(f"Failed to process alert {alert.id}: {e}")
            return {"alert_id": alert.id, "status": "failed", "error": str(e)}
    
    async def enrich_threat_context(self, alert: ThreatAlert) -> Dict[str, Any]:
        """Enrich alert with additional threat intelligence"""
        enrichment = {}
        
        # Geographic enrichment
        try:
            geo_data = await self.get_geolocation_data(alert.src_ip)
            enrichment['geo_data'] = geo_data
        except Exception as e:
            self.logger.warning(f"Geo enrichment failed: {e}")
        
        # Reputation check
        try:
            reputation = await self.check_ip_reputation(alert.src_ip)
            enrichment['reputation'] = reputation
        except Exception as e:
            self.logger.warning(f"Reputation check failed: {e}")
        
        return enrichment
    
    async def determine_response_plan(self, alert: ThreatAlert, enrichment: Dict) -> List[ActionType]:
        """Determine appropriate response actions"""
        actions = []
        
        # Rule-based decision making
        if alert.severity == AlertSeverity.CRITICAL or alert.threat_score > 0.9:
            actions.extend([
                ActionType.BLOCK_IP,
                ActionType.CREATE_INCIDENT,
                ActionType.ESCALATE_HUMAN
            ])
        elif alert.severity == AlertSeverity.HIGH or alert.threat_score > 0.8:
            actions.extend([
                ActionType.CREATE_INCIDENT,
                ActionType.ESCALATE_HUMAN
            ])
        elif alert.severity == AlertSeverity.MEDIUM or alert.threat_score > 0.6:
            actions.append(ActionType.CREATE_INCIDENT)
        else:
            actions.append(ActionType.LOG_SUSPICIOUS)
        
        return actions
    
    async def execute_response_actions(self, alert: ThreatAlert, actions: List[ActionType]) -> Dict[str, Any]:
        """Execute the determined response actions"""
        results = {}
        
        for action in actions:
            try:
                if action == ActionType.BLOCK_IP:
                    result = await self.block_ip_address(alert.src_ip)
                elif action == ActionType.CREATE_INCIDENT:
                    result = await self.create_security_incident(alert)
                elif action == ActionType.ESCALATE_HUMAN:
                    result = await self.escalate_to_human(alert)
                elif action == ActionType.LOG_SUSPICIOUS:
                    result = await self.log_suspicious_activity(alert)
                else:
                    result = {"status": "unknown_action"}
                
                results[action.value] = result
                
            except Exception as e:
                self.logger.error(f"Failed to execute action {action.value}: {e}")
                results[action.value] = {"status": "failed", "error": str(e)}
        
        return results
    
    async def get_geolocation_data(self, ip: str) -> Dict[str, Any]:
        """Get geolocation data for IP address"""
        # Mock implementation - replace with actual geo service
        return {
            "country": "US",
            "city": "Unknown",
            "coordinates": [0.0, 0.0]
        }
    
    async def check_ip_reputation(self, ip: str) -> Dict[str, Any]:
        """Check IP reputation against threat feeds"""
        # Mock implementation - replace with actual reputation service
        return {
            "reputation_score": 0.5,
            "is_malicious": False,
            "sources": ["mock_feed"]
        }
    
    async def block_ip_address(self, ip: str) -> Dict[str, Any]:
        """Block IP address in firewall/WAF"""
        self.logger.info(f"Blocking IP {ip}")
        return {"status": "success", "ip": ip, "blocked_until": (datetime.now() + timedelta(hours=1)).isoformat()}
    
    async def create_security_incident(self, alert: ThreatAlert) -> Dict[str, Any]:
        """Create incident in SIEM/ticketing system"""
        incident_data = {
            "title": f"Cybersecurity Threat Detected - {alert.severity.value.upper()}",
            "description": f"Automated detection of suspicious activity from {alert.src_ip}",
            "severity": alert.severity.value,
            "timestamp": alert.timestamp.isoformat()
        }
        
        self.logger.info(f"Created incident for alert {alert.id}")
        return {"status": "success", "incident_id": f"INC-{alert.id}"}
    
    async def escalate_to_human(self, alert: ThreatAlert) -> Dict[str, Any]:
        """Escalate alert to human analysts"""
        self.logger.info(f"Escalating alert {alert.id} to human review")
        return {"status": "success", "escalated_at": datetime.now().isoformat()}
    
    async def log_suspicious_activity(self, alert: ThreatAlert) -> Dict[str, Any]:
        """Log suspicious activity for future analysis"""
        self.logger.info(f"Logging suspicious activity for alert {alert.id}")
        return {"status": "success", "logged_at": datetime.now().isoformat()}

# Usage example
async def main():
    """Example usage of the triage agent"""
    config = {
        'redis': {'host': 'localhost', 'port': 6379, 'db': 0}
    }
    
    agent = ThreatTriageAgent(config)
    
    # Create sample alert
    alert = ThreatAlert(
        id="alert_001",
        src_ip="192.168.1.100",
        dst_ip="10.0.0.1",
        threat_score=0.85,
        severity=AlertSeverity.HIGH,
        timestamp=datetime.now(),
        features={"bytes_sent": 1000, "packets_sent": 10},
        contributing_factors=["unusual_port", "high_traffic"],
        raw_event={}
    )
    
    result = await agent.process_alert(alert)
    print(f"Processing result: {result}")

if __name__ == "__main__":
    asyncio.run(main())</code></pre>
                    </div>
                </div>

                <!-- Maintenance Agent Tab -->
                <div id="agents-maintenance" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Maintenance Agent</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/agents/maintenance_agent.py
import logging
import time
import asyncio
from typing import Dict, List, Any
from datetime import datetime, timedelta
import mlflow
import requests
import json

class MaintenanceAgent:
    """
    Autonomous agent for system maintenance and model management
    """
    
    def __init__(self, config: Dict):
        self.config = config
        self.logger = logging.getLogger(__name__)
        self.last_check = datetime.now()
    
    async def run_maintenance_cycle(self) -> Dict[str, Any]:
        """Run complete maintenance cycle"""
        self.logger.info("Starting maintenance cycle...")
        
        results = {
            "timestamp": datetime.now().isoformat(),
            "checks_performed": [],
            "actions_taken": [],
            "status": "success"
        }
        
        try:
            # 1. Check model performance
            model_health = await self.check_model_health()
            results["checks_performed"].append("model_health")
            
            if model_health["needs_attention"]:
                action = await self.handle_model_degradation(model_health)
                results["actions_taken"].append(action)
            
            # 2. Check data drift
            drift_status = await self.check_data_drift()
            results["checks_performed"].append("data_drift")
            
            if drift_status["drift_detected"]:
                action = await self.handle_data_drift(drift_status)
                results["actions_taken"].append(action)
            
            # 3. System health check
            system_health = await self.check_system_health()
            results["checks_performed"].append("system_health")
            
            # 4. Clean up old data
            cleanup_result = await self.cleanup_old_data()
            results["actions_taken"].append(cleanup_result)
            
            self.last_check = datetime.now()
            
        except Exception as e:
            self.logger.error(f"Maintenance cycle failed: {e}")
            results["status"] = "failed"
            results["error"] = str(e)
        
        return results
    
    async def check_model_health(self) -> Dict[str, Any]:
        """Check model performance metrics"""
        try:
            # Query MLflow for recent model metrics
            client = mlflow.tracking.MlflowClient()
            
            # Get latest model metrics
            experiments = client.search_experiments()
            if not experiments:
                return {"needs_attention": False, "reason": "No experiments found"}
            
            latest_runs = client.search_runs(
                experiment_ids=[experiments[0].experiment_id],
                order_by=["start_time DESC"],
                max_results=5
            )
            
            if not latest_runs:
                return {"needs_attention": True, "reason": "No recent runs found"}
            
            # Check recent AUC scores
            recent_aucs = []
            for run in latest_runs:
                if "validation_auc" in run.data.metrics:
                    recent_aucs.append(run.data.metrics["validation_auc"])
            
            if not recent_aucs:
                return {"needs_attention": True, "reason": "No AUC metrics found"}
            
            avg_auc = sum(recent_aucs) / len(recent_aucs)
            threshold = self.config.get("model_health", {}).get("min_auc", 0.85)
            
            return {
                "needs_attention": avg_auc < threshold,
                "average_auc": avg_auc,
                "threshold": threshold,
                "recent_runs": len(recent_aucs)
            }
            
        except Exception as e:
            self.logger.error(f"Model health check failed: {e}")
            return {"needs_attention": True, "reason": f"Health check failed: {e}"}
    
    async def check_data_drift(self) -> Dict[str, Any]:
        """Check for data drift in recent predictions"""
        try:
            # This is a simplified drift detection
            # In production, use tools like Evidently or WhyLabs
            
            drift_threshold = self.config.get("drift", {}).get("threshold", 0.1)
            
            # Mock drift calculation
            # Replace with actual drift detection logic
            drift_score = 0.05  # Mock value
            
            return {
                "drift_detected": drift_score > drift_threshold,
                "drift_score": drift_score,
                "threshold": drift_threshold
            }
            
        except Exception as e:
            self.logger.error(f"Data drift check failed: {e}")
            return {"drift_detected": False, "error": str(e)}
    
    async def check_system_health(self) -> Dict[str, Any]:
        """Check overall system health"""
        health_status = {
            "api_healthy": False,
            "redis_healthy": False,
            "model_loaded": False
        }
        
        try:
            # Check API health
            api_url = self.config.get("api_url", "http://localhost:8080")
            response = requests.get(f"{api_url}/health", timeout=5)
            if response.status_code == 200:
                health_data = response.json()
                health_status["api_healthy"] = health_data.get("status") == "healthy"
                health_status["model_loaded"] = health_data.get("model_loaded", False)
                health_status["redis_healthy"] = health_data.get("redis_connected", False)
        
        except Exception as e:
            self.logger.warning(f"API health check failed: {e}")
        
        return health_status
    
    async def handle_model_degradation(self, health_info: Dict) -> Dict[str, Any]:
        """Handle model performance degradation"""
        self.logger.warning(f"Model degradation detected: {health_info}")
        
        # Trigger retraining workflow
        try:
            # This would trigger your retraining pipeline
            # For example, calling Airflow DAG or Kubeflow pipeline
            
            retrain_url = self.config.get("retrain_endpoint")
            if retrain_url:
                response = requests.post(retrain_url, json={"reason": "performance_degradation"})
                
                if response.status_code == 200:
                    return {
                        "action": "trigger_retrain",
                        "status": "success",
                        "job_id": response.json().get("job_id")
                    }
            
            return {
                "action": "trigger_retrain",
                "status": "skipped",
                "reason": "No retrain endpoint configured"
            }
            
        except Exception as e:
            self.logger.error(f"Failed to trigger retraining: {e}")
            return {
                "action": "trigger_retrain",
                "status": "failed",
                "error": str(e)
            }
    
    async def handle_data_drift(self, drift_info: Dict) -> Dict[str, Any]:
        """Handle detected data drift"""
        self.logger.warning(f"Data drift detected: {drift_info}")
        
        # Send alert to monitoring system
        try:
            webhook_url = self.config.get("alert_webhook")
            if webhook_url:
                alert_data = {
                    "type": "data_drift",
                    "severity": "medium",
                    "message": f"Data drift detected with score {drift_info['drift_score']:.3f}",
                    "timestamp": datetime.now().isoformat()
                }
                
                response = requests.post(webhook_url, json=alert_data)
                
                return {
                    "action": "send_drift_alert",
                    "status": "success" if response.status_code == 200 else "failed"
                }
            
            return {
                "action": "send_drift_alert",
                "status": "skipped",
                "reason": "No webhook configured"
            }
            
        except Exception as e:
            return {
                "action": "send_drift_alert",
                "status": "failed",
                "error": str(e)
            }
    
    async def cleanup_old_data(self) -> Dict[str, Any]:
        """Clean up old logs and temporary data"""
        try:
            cleanup_stats = {
                "logs_cleaned": 0,
                "temp_files_removed": 0,
                "old_models_archived": 0
            }
            
            # This would implement actual cleanup logic
            # For example, removing old log files, temporary model artifacts, etc.
            
            self.logger.info("Cleanup completed")
            
            return {
                "action": "cleanup_old_data",
                "status": "success",
                "stats": cleanup_stats
            }
            
        except Exception as e:
            return {
                "action": "cleanup_old_data",
                "status": "failed",
                "error": str(e)
            }

# Scheduler for maintenance agent
class MaintenanceScheduler:
    """Scheduler for running maintenance tasks"""
    
    def __init__(self, agent: MaintenanceAgent, interval_hours: int = 6):
        self.agent = agent
        self.interval_hours = interval_hours
        self.running = False
    
    async def start(self):
        """Start the maintenance scheduler"""
        self.running = True
        while self.running:
            try:
                await self.agent.run_maintenance_cycle()
                await asyncio.sleep(self.interval_hours * 3600)  # Convert to seconds
            except Exception as e:
                logging.error(f"Maintenance scheduler error: {e}")
                await asyncio.sleep(300)  # Wait 5 minutes before retry
    
    def stop(self):
        """Stop the maintenance scheduler"""
        self.running = False

# Usage example
async def main():
    """Example usage of the maintenance agent"""
    config = {
        "model_health": {"min_auc": 0.85},
        "drift": {"threshold": 0.1},
        "api_url": "http://localhost:8080",
        "retrain_endpoint": "http://localhost:8081/trigger-retrain",
        "alert_webhook": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL"
    }
    
    agent = MaintenanceAgent(config)
    result = await agent.run_maintenance_cycle()
    print(f"Maintenance result: {result}")

if __name__ == "__main__":
    asyncio.run(main())</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Deployment Section -->
        <section id="deployment" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">Deployment & Infrastructure</h2>
                
                <!-- Tab Navigation -->
                <div class="flex space-x-4 mb-6 border-b">
                    <button onclick="showTab('deployment', 'docker')" class="tab-button px-4 py-2 font-medium text-blue-600 border-b-2 border-blue-600 active">Docker</button>
                    <button onclick="showTab('deployment', 'kubernetes')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Kubernetes</button>
                    <button onclick="showTab('deployment', 'cicd')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">CI/CD</button>
                </div>

                <!-- Docker Tab -->
                <div id="deployment-docker" class="tab-content active">
                    <h3 class="text-xl font-semibold mb-4">Docker Configuration</h3>
                    <div class="mb-4">
                        <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/08cdb1ef-b61b-41fa-973c-55dd7da010a6.png" alt="Docker containerization diagram showing application containers, model serving containers, Redis cache, and load balancer with blue Docker whale logo styling" class="w-full rounded-lg shadow-md">
                    </div>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-dockerfile"># Dockerfile
FROM python:3.11-slim

WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt ./
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY src/ ./src/
COPY models/ ./models/
COPY configs/ ./configs/

# Create non-root user
RUN useradd -m -u 1000 appuser && chown -R appuser:appuser /app
USER appuser

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

# Expose port
EXPOSE 8080

# Start application
CMD ["uvicorn", "src.app:app", "--host", "0.0.0.0", "--port", "8080", "--workers", "1"]</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Docker Compose</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># docker-compose.yml
version: '3.8'

services:
  cybersecurity-api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - REDIS_HOST=redis
      - REDIS_PORT=6379
      - MLFLOW_TRACKING_URI=http://mlflow:5000
    depends_on:
      - redis
      - mlflow
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: unless-stopped
    volumes:
      - ./models:/app/models:ro
      - ./logs:/app/logs

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    command: redis-server --appendonly yes
    volumes:
      - redis_data:/data
    restart: unless-stopped

  mlflow:
    image: python:3.11-slim
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    command: >
      bash -c "pip install mlflow==2.7.0 &&
               mlflow server 
               --backend-store-uri sqlite:///mlflow/mlflow.db 
               --default-artifact-root /mlflow/artifacts 
               --host 0.0.0.0 
               --port 5000"
    volumes:
      - mlflow_data:/mlflow
    restart: unless-stopped

  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    restart: unless-stopped

  grafana:
    image: grafana/grafana:latest
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana:/etc/grafana/provisioning
    restart: unless-stopped

volumes:
  redis_data:
  mlflow_data:
  prometheus_data:
  grafana_data:

networks:
  default:
    driver: bridge</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Build & Run Commands</h4>
                    <div class="bg-gray-100 rounded-lg p-4">
                        <div class="space-y-2 text-sm">
                            <div><strong>Build Docker Image:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">docker build -t cybersecurity-ml:latest .</code>
                            
                            <div class="mt-4"><strong>Run with Docker Compose:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">docker-compose up -d</code>
                            
                            <div class="mt-4"><strong>View Logs:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">docker-compose logs -f cybersecurity-api</code>
                            
                            <div class="mt-4"><strong>Scale Services:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">docker-compose up -d --scale cybersecurity-api=3</code>
                        </div>
                    </div>
                </div>

                <!-- Kubernetes Tab -->
                <div id="deployment-kubernetes" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Kubernetes Manifests</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># deploy/k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cybersecurity-api
  labels:
    app: cybersecurity-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cybersecurity-api
  template:
    metadata:
      labels:
        app: cybersecurity-api
    spec:
      containers:
      - name: api
        image: cybersecurity-ml:latest
        ports:
        - containerPort: 8080
        env:
        - name: REDIS_HOST
          value: "redis-service"
        - name: REDIS_PORT
          value: "6379"
        - name: MLFLOW_TRACKING_URI
          value: "http://mlflow-service:5000"
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 30
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        volumeMounts:
        - name: model-storage
          mountPath: /app/models
          readOnly: true
      volumes:
      - name: model-storage
        persistentVolumeClaim:
          claimName: model-storage-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: cybersecurity-api-service
spec:
  selector:
    app: cybersecurity-api
  ports:
  - port: 80
    targetPort: 8080
  type: LoadBalancer
---
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: cybersecurity-api-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: cybersecurity-api
  minReplicas: 2
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Redis & Supporting Services</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># deploy/k8s/redis.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  replicas: 1
  selector:
    matchLabels:
      app: redis
  template:
    metadata:
      labels:
        app: redis
    spec:
      containers:
      - name: redis
        image: redis:7-alpine
        ports:
        - containerPort: 6379
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "200m"
        volumeMounts:
        - name: redis-storage
          mountPath: /data
      volumes:
      - name: redis-storage
        persistentVolumeClaim:
          claimName: redis-storage-pvc
---
apiVersion: v1
kind: Service
metadata:
  name: redis-service
spec:
  selector:
    app: redis
  ports:
  - port: 6379
    targetPort: 6379
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: redis-storage-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: model-storage-pvc
spec:
  accessModes:
    - ReadOnlyMany
  resources:
    requests:
      storage: 5Gi</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Deployment Commands</h4>
                    <div class="bg-gray-100 rounded-lg p-4">
                        <div class="space-y-2 text-sm">
                            <div><strong>Apply Manifests:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">kubectl apply -f deploy/k8s/</code>
                            
                            <div class="mt-4"><strong>Check Status:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">kubectl get pods -l app=cybersecurity-api</code>
                            
                            <div class="mt-4"><strong>View Logs:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">kubectl logs -f deployment/cybersecurity-api</code>
                            
                            <div class="mt-4"><strong>Port Forward for Testing:</strong></div>
                            <code class="block bg-white px-3 py-2 rounded">kubectl port-forward service/cybersecurity-api-service 8080:80</code>
                        </div>
                    </div>
                </div>

                <!-- CI/CD Tab -->
                <div id="deployment-cicd" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">CI/CD Pipeline</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># .github/workflows/ci-cd.yml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Lint with flake8
      run: |
        flake8 src/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
    
    - name: Type check with mypy
      run: mypy src/
    
    - name: Test with pytest
      run: |
        pytest tests/ -v --cov=src --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3

  build-and-push:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'push'
    
    permissions:
      contents: read
      packages: write
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v3
    
    - name: Log in to Container Registry
      uses: docker/login-action@v2
      with:
        registry: ${{ env.REGISTRY }}
        username: ${{ github.actor }}
        password: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v4
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=sha,prefix={{branch}}-
    
    - name: Build and push Docker image
      uses: docker/build-push-action@v4
      with:
        context: .
        push: true
        tags: ${{ steps.meta.outputs.tags }}
        labels: ${{ steps.meta.outputs.labels }}

  deploy-staging:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - name: Deploy to Staging
      run: |
        echo "Deploying to staging environment..."
        # Add your staging deployment commands here
        # kubectl set image deployment/cybersecurity-api api=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:develop

  deploy-production:
    needs: build-and-push
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/main'
    environment: production
    
    steps:
    - name: Deploy to Production
      run: |
        echo "Deploying to production environment..."
        # Add your production deployment commands here
        # kubectl set image deployment/cybersecurity-api api=${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:main</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Makefile for Development</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-makefile"># Makefile
.PHONY: help install test lint format build run clean

help:
	@echo "Available commands:"
	@echo "  install    Install dependencies"
	@echo "  test       Run tests"
	@echo "  lint       Run linting"
	@echo "  format     Format code"
	@echo "  build      Build Docker image"
	@echo "  run        Run application"
	@echo "  clean      Clean up"

install:
	pip install -r requirements.txt
	pip install -r requirements-dev.txt

test:
	pytest tests/ -v --cov=src --cov-report=html

lint:
	flake8 src/ tests/
	mypy src/

format:
	black src/ tests/
	isort src/ tests/

build:
	docker build -t cybersecurity-ml:latest .

run:
	uvicorn src.app:app --reload --host 0.0.0.0 --port 8080

run-docker:
	docker-compose up -d

stop-docker:
	docker-compose down

train:
	python src/train.py --data data/cybersecurity_data.csv

clean:
	find . -type f -name "*.pyc" -delete
	find . -type d -name "__pycache__" -delete
	docker system prune -f</code></pre>
                    </div>
                </div>
            </div>
        </section>

        <!-- Monitoring Section -->
        <section id="monitoring" class="section-content">
            <div class="bg-white rounded-lg shadow-lg p-8 mb-8">
                <h2 class="text-3xl font-bold mb-6 text-gray-800">Monitoring & Observability</h2>
                
                <!-- Tab Navigation -->
                <div class="flex space-x-4 mb-6 border-b">
                    <button onclick="showTab('monitoring', 'metrics')" class="tab-button px-4 py-2 font-medium text-blue-600 border-b-2 border-blue-600 active">Metrics</button>
                    <button onclick="showTab('monitoring', 'logging')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Logging</button>
                    <button onclick="showTab('monitoring', 'alerting')" class="tab-button px-4 py-2 font-medium text-gray-500 hover:text-gray-700">Alerting</button>
                </div>

                <!-- Metrics Tab -->
                <div id="monitoring-metrics" class="tab-content active">
                    <h3 class="text-xl font-semibold mb-4">Prometheus Metrics & Grafana Dashboards</h3>
                    <div class="mb-4">
                        <img src="https://storage.googleapis.com/workspace-0f70711f-8b4e-4d94-86f1-2a93ccde5887/image/6dbd7d0e-2894-416e-b1db-a470761e1316.png" alt="Grafana dashboard showing cybersecurity metrics with graphs for prediction latency, threat scores, model performance, and system health indicators in a dark theme" class="w-full rounded-lg shadow-md">
                    </div>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "alert_rules.yml"

scrape_configs:
  - job_name: 'cybersecurity-api'
    static_configs:
      - targets: ['cybersecurity-api:8080']
    metrics_path: '/metrics'
    scrape_interval: 10s

  - job_name: 'redis'
    static_configs:
      - targets: ['redis:6379']

  - job_name: 'prometheus'
    static_configs:
      - targets: ['localhost:9090']

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Custom Metrics Implementation</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/utils/monitoring.py
from prometheus_client import Counter, Histogram, Gauge, CollectorRegistry
import time
import logging
from typing import Dict, Any
from functools import wraps

# Create custom registry
REGISTRY = CollectorRegistry()

# Define metrics
PREDICTION_REQUESTS = Counter(
    'prediction_requests_total',
    'Total prediction requests',
    ['model_version', 'endpoint'],
    registry=REGISTRY
)

PREDICTION_LATENCY = Histogram(
    'prediction_latency_seconds',
    'Prediction latency in seconds',
    ['model_version'],
    registry=REGISTRY
)

THREAT_SCORE_DISTRIBUTION = Histogram(
    'threat_score_distribution',
    'Distribution of threat scores',
    buckets=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],
    registry=REGISTRY
)

ANOMALY_DETECTIONS = Counter(
    'anomaly_detections_total',
    'Total anomaly detections',
    ['severity', 'source'],
    registry=REGISTRY
)

MODEL_HEALTH = Gauge(
    'model_health_score',
    'Current model health score',
    ['model_name'],
    registry=REGISTRY
)

ACTIVE_CONNECTIONS = Gauge(
    'active_connections',
    'Number of active connections',
    registry=REGISTRY
)

DATA_DRIFT_SCORE = Gauge(
    'data_drift_score',
    'Current data drift score',
    ['feature_group'],
    registry=REGISTRY
)

class MetricsCollector:
    """Centralized metrics collection"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def record_prediction(self, latency: float, threat_score: float, 
                         model_version: str = "v1.0", endpoint: str = "predict"):
        """Record prediction metrics"""
        PREDICTION_REQUESTS.labels(
            model_version=model_version,
            endpoint=endpoint
        ).inc()
        
        PREDICTION_LATENCY.labels(
            model_version=model_version
        ).observe(latency)
        
        THREAT_SCORE_DISTRIBUTION.observe(threat_score)
    
    def record_anomaly(self, severity: str, source: str = "model"):
        """Record anomaly detection"""
        ANOMALY_DETECTIONS.labels(
            severity=severity,
            source=source
        ).inc()
    
    def update_model_health(self, model_name: str, health_score: float):
        """Update model health score"""
        MODEL_HEALTH.labels(model_name=model_name).set(health_score)
    
    def update_drift_score(self, feature_group: str, drift_score: float):
        """Update data drift score"""
        DATA_DRIFT_SCORE.labels(feature_group=feature_group).set(drift_score)

# Decorator for monitoring function performance
def monitor_performance(metric_name: str = None):
    """Decorator to monitor function performance"""
    def decorator(func):
        @wraps(func)
        async def async_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = await func(*args, **kwargs)
                latency = time.time() - start_time
                
                # Record metrics if collector available
                if hasattr(func, '_metrics_collector'):
                    func._metrics_collector.record_latency(
                        metric_name or func.__name__, 
                        latency
                    )
                
                return result
            except Exception as e:
                # Record error metrics
                logging.error(f"Error in {func.__name__}: {e}")
                raise
        
        @wraps(func)
        def sync_wrapper(*args, **kwargs):
            start_time = time.time()
            try:
                result = func(*args, **kwargs)
                latency = time.time() - start_time
                return result
            except Exception as e:
                logging.error(f"Error in {func.__name__}: {e}")
                raise
        
        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper
    return decorator

# Global metrics collector instance
metrics_collector = MetricsCollector()</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Grafana Dashboard Configuration</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-json">{
  "dashboard": {
    "title": "Cybersecurity ML Pipeline",
    "panels": [
      {
        "title": "Prediction Requests per Second",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(prediction_requests_total[5m])",
            "legendFormat": "{{model_version}} - {{endpoint}}"
          }
        ]
      },
      {
        "title": "Prediction Latency",
        "type": "graph",
        "targets": [
          {
            "expr": "histogram_quantile(0.95, rate(prediction_latency_seconds_bucket[5m]))",
            "legendFormat": "95th percentile"
          },
          {
            "expr": "histogram_quantile(0.50, rate(prediction_latency_seconds_bucket[5m]))",
            "legendFormat": "50th percentile"
          }
        ]
      },
      {
        "title": "Threat Score Distribution",
        "type": "heatmap",
        "targets": [
          {
            "expr": "rate(threat_score_distribution_bucket[5m])",
            "format": "heatmap"
          }
        ]
      },
      {
        "title": "Anomaly Detections",
        "type": "stat",
        "targets": [
          {
            "expr": "increase(anomaly_detections_total[1h])",
            "legendFormat": "{{severity}}"
          }
        ]
      },
      {
        "title": "Model Health Score",
        "type": "gauge",
        "targets": [
          {
            "expr": "model_health_score",
            "legendFormat": "{{model_name}}"
          }
        ]
      },
      {
        "title": "Data Drift Score",
        "type": "graph",
        "targets": [
          {
            "expr": "data_drift_score",
            "legendFormat": "{{feature_group}}"
          }
        ]
      }
    ]
  }
}</code></pre>
                    </div>
                </div>

                <!-- Logging Tab -->
                <div id="monitoring-logging" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Structured Logging & Log Analysis</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/utils/logging_config.py
import logging
import logging.config
import json
import sys
from datetime import datetime
from typing import Dict, Any
import traceback

class JSONFormatter(logging.Formatter):
    """Custom JSON formatter for structured logging"""
    
    def format(self, record):
        log_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "level": record.levelname,
            "logger": record.name,
            "message": record.getMessage(),
            "module": record.module,
            "function": record.funcName,
            "line": record.lineno
        }
        
        # Add exception info if present
        if record.exc_info:
            log_entry["exception"] = {
                "type": record.exc_info[0].__name__,
                "message": str(record.exc_info[1]),
                "traceback": traceback.format_exception(*record.exc_info)
            }
        
        # Add extra fields
        if hasattr(record, 'extra_fields'):
            log_entry.update(record.extra_fields)
        
        return json.dumps(log_entry)

class CybersecurityLogger:
    """Centralized logging for cybersecurity events"""
    
    def __init__(self, name: str = "cybersecurity"):
        self.logger = logging.getLogger(name)
        self._setup_logging()
    
    def _setup_logging(self):
        """Setup logging configuration"""
        logging_config = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "json": {
                    "()": JSONFormatter
                },
                "standard": {
                    "format": "%(asctime)s [%(levelname)s] %(name)s: %(message)s"
                }
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "level": "INFO",
                    "formatter": "json",
                    "stream": sys.stdout
                },
                "file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "level": "DEBUG",
                    "formatter": "json",
                    "filename": "logs/cybersecurity.log",
                    "maxBytes": 10485760,  # 10MB
                    "backupCount": 5
                },
                "error_file": {
                    "class": "logging.handlers.RotatingFileHandler",
                    "level": "ERROR",
                    "formatter": "json",
                    "filename": "logs/errors.log",
                    "maxBytes": 10485760,
                    "backupCount": 5
                }
            },
            "loggers": {
                "cybersecurity": {
                    "level": "DEBUG",
                    "handlers": ["console", "file", "error_file"],
                    "propagate": False
                },
                "uvicorn": {
                    "level": "INFO",
                    "handlers": ["console"],
                    "propagate": False
                }
            },
            "root": {
                "level": "INFO",
                "handlers": ["console"]
            }
        }
        
        logging.config.dictConfig(logging_config)
    
    def log_prediction(self, event_id: str, threat_score: float, 
                      latency: float, model_version: str):
        """Log prediction event"""
        extra_fields = {
            "event_type": "prediction",
            "event_id": event_id,
            "threat_score": threat_score,
            "latency_ms": latency * 1000,
            "model_version": model_version
        }
        
        self.logger.info(
            f"Prediction completed for event {event_id}",
            extra={"extra_fields": extra_fields}
        )
    
    def log_anomaly_detection(self, event_id: str, severity: str, 
                            src_ip: str, contributing_factors: list):
        """Log anomaly detection"""
        extra_fields = {
            "event_type": "anomaly_detection",
            "event_id": event_id,
            "severity": severity,
            "src_ip": src_ip,
            "contributing_factors": contributing_factors
        }
        
        self.logger.warning(
            f"Anomaly detected: {severity} severity from {src_ip}",
            extra={"extra_fields": extra_fields}
        )
    
    def log_model_performance(self, model_name: str, metrics: Dict[str, float]):
        """Log model performance metrics"""
        extra_fields = {
            "event_type": "model_performance",
            "model_name": model_name,
            "metrics": metrics
        }
        
        self.logger.info(
            f"Model performance update for {model_name}",
            extra={"extra_fields": extra_fields}
        )
    
    def log_security_event(self, event_type: str, details: Dict[str, Any]):
        """Log security-related events"""
        extra_fields = {
            "event_type": "security_event",
            "security_event_type": event_type,
            "details": details
        }
        
        self.logger.warning(
            f"Security event: {event_type}",
            extra={"extra_fields": extra_fields}
        )

# Global logger instance
cyber_logger = CybersecurityLogger()

# Log analysis queries for ELK stack or similar
USEFUL_LOG_QUERIES = {
    "high_threat_predictions": {
        "query": "event_type:prediction AND threat_score:>0.8",
        "description": "Find high-threat predictions"
    },
    "slow_predictions": {
        "query": "event_type:prediction AND latency_ms:>1000",
        "description": "Find slow predictions"
    },
    "model_errors": {
        "query": "level:ERROR AND logger:cybersecurity",
        "description": "Find model-related errors"
    },
    "anomaly_trends": {
        "query": "event_type:anomaly_detection",
        "aggregation": "terms",
        "field": "severity",
        "description": "Analyze anomaly severity trends"
    },
    "ip_activity": {
        "query": "src_ip:* AND event_type:anomaly_detection",
        "aggregation": "terms",
        "field": "src_ip",
        "description": "Top source IPs triggering anomalies"
    }
}</code></pre>
                    </div>
                </div>

                <!-- Alerting Tab -->
                <div id="monitoring-alerting" class="tab-content">
                    <h3 class="text-xl font-semibold mb-4">Alerting Rules & Escalation</h3>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># monitoring/alert_rules.yml
groups:
- name: cybersecurity.rules
  rules:
  - alert: HighPredictionLatency
    expr: histogram_quantile(0.95, rate(prediction_latency_seconds_bucket[5m])) > 1.0
    for: 2m
    labels:
      severity: warning
      component: api
    annotations:
      summary: "High prediction latency detected"
      description: "95th percentile prediction latency is {{ $value }}s"

  - alert: ModelHealthDegraded
    expr: model_health_score < 0.8
    for: 5m
    labels:
      severity: critical
      component: model
    annotations:
      summary: "Model health degraded"
      description: "Model {{ $labels.model_name }} health score is {{ $value }}"

  - alert: HighAnomalyRate
    expr: rate(anomaly_detections_total[5m]) > 10
    for: 1m
    labels:
      severity: warning
      component: detection
    annotations:
      summary: "High anomaly detection rate"
      description: "Anomaly detection rate is {{ $value }} per second"

  - alert: CriticalAnomalyDetected
    expr: increase(anomaly_detections_total{severity="critical"}[1m]) > 0
    for: 0m
    labels:
      severity: critical
      component: security
    annotations:
      summary: "Critical security anomaly detected"
      description: "{{ $value }} critical anomalies detected in the last minute"

  - alert: DataDriftDetected
    expr: data_drift_score > 0.3
    for: 10m
    labels:
      severity: warning
      component: data
    annotations:
      summary: "Data drift detected"
      description: "Data drift score for {{ $labels.feature_group }} is {{ $value }}"

  - alert: APIDown
    expr: up{job="cybersecurity-api"} == 0
    for: 1m
    labels:
      severity: critical
      component: api
    annotations:
      summary: "Cybersecurity API is down"
      description: "The cybersecurity API has been down for more than 1 minute"

  - alert: HighErrorRate
    expr: rate(prediction_requests_total{status="error"}[5m]) / rate(prediction_requests_total[5m]) > 0.1
    for: 2m
    labels:
      severity: warning
      component: api
    annotations:
      summary: "High error rate detected"
      description: "Error rate is {{ $value | humanizePercentage }}"</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Alertmanager Configuration</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-yaml"># monitoring/alertmanager.yml
global:
  smtp_smarthost: 'localhost:587'
  smtp_from: 'alerts@company.com'

route:
  group_by: ['alertname', 'component']
  group_wait: 10s
  group_interval: 10s
  repeat_interval: 1h
  receiver: 'default'
  routes:
  - match:
      severity: critical
    receiver: 'critical-alerts'
    group_wait: 0s
    repeat_interval: 5m
  - match:
      component: security
    receiver: 'security-team'
    group_wait: 0s

receivers:
- name: 'default'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#cybersecurity-alerts'
    title: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'

- name: 'critical-alerts'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#critical-alerts'
    title: '🚨 CRITICAL: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  email_configs:
  - to: 'oncall@company.com'
    subject: 'CRITICAL: Cybersecurity Alert'
    body: |
      {{ range .Alerts }}
      Alert: {{ .Annotations.summary }}
      Description: {{ .Annotations.description }}
      Severity: {{ .Labels.severity }}
      Component: {{ .Labels.component }}
      {{ end }}

- name: 'security-team'
  slack_configs:
  - api_url: 'YOUR_SLACK_WEBHOOK_URL'
    channel: '#security-incidents'
    title: '🔒 Security Alert: {{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
    text: '{{ range .Alerts }}{{ .Annotations.description }}{{ end }}'
  webhook_configs:
  - url: 'https://your-siem-system.com/api/alerts'
    send_resolved: true

inhibit_rules:
- source_match:
    severity: 'critical'
  target_match:
    severity: 'warning'
  equal: ['alertname', 'component']</code></pre>
                    </div>
                    
                    <h4 class="text-lg font-semibold mb-3">Custom Alert Handler</h4>
                    <div class="code-container bg-gray-900 rounded-lg p-4 mb-4">
                        <pre><code class="language-python"># src/utils/alert_handler.py
import asyncio
import logging
import requests
import json
from typing import Dict, List, Any
from datetime import datetime
from enum import Enum

class AlertSeverity(Enum):
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"

class AlertHandler:
    """Handle and route alerts based on severity and type"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.logger = logging.getLogger(__name__)
    
    async def send_alert(self, alert_type: str, severity: AlertSeverity, 
                        message: str, details: Dict[str, Any] = None):
        """Send alert through configured channels"""
        alert_data = {
            "timestamp": datetime.utcnow().isoformat(),
            "type": alert_type,
            "severity": severity.value,
            "message": message,
            "details": details or {}
        }
        
        # Route based on severity
        if severity == AlertSeverity.CRITICAL:
            await self._send_critical_alert(alert_data)
        elif severity == AlertSeverity.WARNING:
            await self._send_warning_alert(alert_data)
        else:
            await self._send_info_alert(alert_data)
    
    async def _send_critical_alert(self, alert_data: Dict[str, Any]):
        """Send critical alerts through all channels"""
        tasks = []
        
        # Slack
        if "slack" in self.config:
            tasks.append(self._send_slack_alert(alert_data, urgent=True))
        
        # Email
        if "email" in self.config:
            tasks.append(self._send_email_alert(alert_data))
        
        # PagerDuty
        if "pagerduty" in self.config:
            tasks.append(self._send_pagerduty_alert(alert_data))
        
        # SIEM
        if "siem" in self.config:
            tasks.append(self._send_siem_alert(alert_data))
        
        await asyncio.gather(*tasks, return_exceptions=True)
    
    async def _send_slack_alert(self, alert_data: Dict[str, Any], urgent: bool = False):
        """Send alert to Slack"""
        try:
            webhook_url = self.config["slack"]["webhook_url"]
            channel = self.config["slack"].get("channel", "#alerts")
            
            color = {
                "critical": "danger",
                "warning": "warning",
                "info": "good"
            }.get(alert_data["severity"], "warning")
            
            icon = "🚨" if urgent else "⚠️"
            
            payload = {
                "channel": channel,
                "text": f"{icon} {alert_data['message']}",
                "attachments": [
                    {
                        "color": color,
                        "fields": [
                            {
                                "title": "Type",
                                "value": alert_data["type"],
                                "short": True
                            },
                            {
                                "title": "Severity",
                                "value": alert_data["severity"].upper(),
                                "short": True
                            },
                            {
                                "title": "Timestamp",
                                "value": alert_data["timestamp"],
                                "short": False
                            }
                        ]
                    }
                ]
            }
            
            response = requests.post(webhook_url, json=payload, timeout=10)
            response.raise_for_status()
            
        except Exception as e:
            self.logger.error(f"Failed to send Slack alert: {e}")
    
    async def _send_email_alert(self, alert_data: Dict[str, Any]):
        """Send alert via email"""
        try:
            import smtplib
            from email.mime.text import MIMEText
            from email.mime.multipart import MIMEMultipart
            
            smtp_config = self.config["email"]
            
            msg = MIMEMultipart()
            msg['From'] = smtp_config["from"]
            msg['To'] = ", ".join(smtp_config["to"])
            msg['Subject'] = f"[{alert_data['severity'].upper()}] {alert_data['type']}: {alert_data['message']}"
            
            body = f"""
            Alert Details:
            
            Type: {alert_data['type']}
            Severity: {alert_data['severity']}
            Message: {alert_data['message']}
            Timestamp: {alert_data['timestamp']}
            
            Details:
            {json.dumps(alert_data['details'], indent=2)}
            """
            
            msg.attach(MIMEText(body, 'plain'))
            
            server = smtplib.SMTP(smtp_config["smtp_server"], smtp_config["port"])
            if smtp_config.get("use_tls"):
                server.starttls()
            if smtp_config.get("username"):
                server.login(smtp_config["username"], smtp_config["password"])
            
            server.send_message(msg)
            server.quit()
            
        except Exception as e:
            self.logger.error(f"Failed to send email alert: {e}")
    
    async def _send_pagerduty_alert(self, alert_data: Dict[str, Any]):
        """Send alert to PagerDuty"""
        try:
            pd_config = self.config["pagerduty"]
            
            payload = {
                "routing_key": pd_config["integration_key"],
                "event_action": "trigger",
                "payload": {
                    "summary": alert_data["message"],
                    "severity": alert_data["severity"],
                    "source": "cybersecurity-ml-pipeline",
                    "custom_details": alert_data["details"]
                }
            }
            
            response = requests.post(
                "https://events.pagerduty.com/v2/enqueue",
                json=payload,
                timeout=10
            )
            response.raise_for_status()
            
        except Exception as e:
            self.logger.error(f"Failed to send PagerDuty alert: {e}")
    
    async def _send_siem_alert(self, alert_data: Dict[str, Any]):
        """Send alert to SIEM system"""
        try:
            siem_config = self.config["siem"]
            
            response = requests.post(
                siem_config["endpoint"],
                json=alert_data,
                headers=siem_config.get("headers", {}),
                timeout=10
            )
            response.raise_for_status()
            
        except Exception as e:
            self.logger.error(f"Failed to send SIEM alert: {e}")

# Usage example
alert_config = {
    "slack": {
        "webhook_url": "https://hooks.slack.com/services/YOUR/WEBHOOK/URL",
        "channel": "#cybersecurity-alerts"
    },
    "email": {
        "smtp_server": "smtp.company.com",
        "port": 587,
        "use_tls": True,
        "from": "alerts@company.com",
        "to": ["security@company.com", "oncall@company.com"],
        "username": "alerts@company.com",
        "password": "your-password"
    },
    "pagerduty": {
        "integration_key": "your-pagerduty-integration-key"
    },
    "siem": {
        "endpoint": "https://your-siem-system.com/api/alerts",
        "headers": {"Authorization": "Bearer your-token"}
    }
}

alert_handler = AlertHandler(alert_config)</code></pre>
                    </div>
                </div>
            </div>
        </section>
    </div>

    <script>
        // Navigation functionality
        function showSection(sectionId) {
            // Hide all sections
            const sections = document.querySelectorAll('.section-content');
            sections.forEach(section => {
                section.classList.remove('active');
            });
            
            // Show selected section
            const targetSection = document.getElementById(sectionId);
            if (targetSection) {
                targetSection.classList.add('active');
            }
            
            // Update navigation buttons
            const navButtons = document.querySelectorAll('.nav-btn');
            navButtons.forEach(btn => {
                btn.classList.remove('active');
            });
            
            // Add active class to clicked button
            event.target.classList.add('active');
        }

        // Tab functionality
        function showTab(section, tabId) {
            // Hide all tab contents in the section
            const tabContents = document.querySelectorAll(`#${section}-${tabId}`).forEach(content => {
                content.parentElement.querySelectorAll('.tab-content').forEach(tab => {
                    tab.classList.remove('active');
                });
            });
            
            // Show selected tab content
            const targetTab = document.getElementById(`${section}-${tabId}`);
            if (targetTab) {
                targetTab.classList.add('active');
            }
            
            // Update tab buttons
            const tabContainer = targetTab.parentElement;
            const tabButtons = tabContainer.parentElement.querySelectorAll('.tab-button');
            tabButtons.forEach(btn => {
                btn.classList.remove('active');
                btn.classList.remove('text-blue-600', 'border-blue-600', 'border-b-2');
                btn.classList.add('text-gray-500', 'hover:text-gray-700');
            });
            
            // Add active class to clicked tab button
            event.target.classList.add('active');
            event.target.classList.add('text-blue-600', 'border-blue-600', 'border-b-2');
            event.target.classList.remove('text-gray-500', 'hover:text-gray-700');
        }

        // Initialize the page
        document.addEventListener('DOMContentLoaded', function() {
            // Show overview section by default
            showSection('overview');
            
            // Initialize syntax highlighting
            if (typeof Prism !== 'undefined') {
                Prism.highlightAll();
            }
        });

        // Smooth scrolling for anchor links
        document.querySelectorAll('a[href^="#"]').forEach(anchor => {
            anchor.addEventListener('click', function (e) {
                e.preventDefault();
                const target = document.querySelector(this.getAttribute('href'));
                if (target) {
                    target.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });
    </script>
</body>
</html>

